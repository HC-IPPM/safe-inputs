steps:
  - id: install
    name: 'node:22-alpine3.21@sha256:f2dc6eea95f787e25f173ba9904c9d0647ab2506178c7b5b7c5a3d02bc4af145'
    dir: api
    entrypoint: npm
    args: ['ci', '--no-optional']

  - id: 'Check types'
    name: 'node:22-alpine3.21@sha256:f2dc6eea95f787e25f173ba9904c9d0647ab2506178c7b5b7c5a3d02bc4af145'
    dir: api
    entrypoint: npm
    args: ['run', 'typecheck']

  - id: 'Run tests'
    name: 'gcr.io/cloud-builders/docker@sha256:b991d50960b337f581ad77ea2a59259a786d177019aa64d8b3acb01f65dbc154'
    script: |
      #!/usr/bin/env bash
      set -o errexit
      set -o pipefail
      set -o nounset

      # Create the coverage directory 
      mkdir -p /workspace/coverage
      chmod -R 777 /workspace/coverage

      # disable TTY output for the compose environment, the ansi characters printed when a TTY is attached
      # are just junk characters to the GCP cloudbuild logs
      DOCKER_NETWORK=cloudbuild DOCKER_TTY=false docker compose -f api/docker-compose.dev-test.yaml up --exit-code-from api-test

      # List the coverage directory contents to confirm files exist
      echo "Listing files in /workspace/coverage:"
      ls -la /workspace/coverage

  - id: 'Upload test coverage report to bucket'
    name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:latest@sha256:3a24ff5f089d9ce3627306873ef1e1061488a63ae12c0bd0b5c24ec5ee594798'
    dir: api
    entrypoint: bash
    args:
      - '-c'
      - |
        # Copy upload script
        cp ../devsecops/code-coverage-reporting/upload-coverage-to-gcs.sh upload-coverage-to-gcs.sh

        # Upload coverage to GCS
        ./upload-coverage-to-gcs.sh \
          --cloudbuild-dir api \
          --coverage-dir /workspace/coverage \
          --branch-name $BRANCH_NAME \
          --short-sha $SHORT_SHA

  - id: generate-image-name
    name: 'gcr.io/cloud-builders/docker@sha256:b991d50960b337f581ad77ea2a59259a786d177019aa64d8b3acb01f65dbc154'
    entrypoint: 'bash'
    dir: api
    args:
      - '-c'
      - |
        echo "northamerica-northeast1-docker.pkg.dev/${PROJECT_ID}/phx-01j1tbke0ax-safeinputs/api:$BRANCH_NAME-$SHORT_SHA-$(date +%s)" > /workspace/imagename

  - name: 'gcr.io/cloud-builders/docker@sha256:b991d50960b337f581ad77ea2a59259a786d177019aa64d8b3acb01f65dbc154'
    id: build-if-main
    entrypoint: 'bash'
    dir: api
    args:
      - '-c'
      - |
        if [[ "$BRANCH_NAME" == "main" ]]
        then
          image=$(cat /workspace/imagename)
          docker build -t $image -f ./Dockerfile.prod .
        else
          exit 0
        fi

  - id: push-if-main
    name: 'gcr.io/cloud-builders/docker@sha256:b991d50960b337f581ad77ea2a59259a786d177019aa64d8b3acb01f65dbc154'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [[ "$BRANCH_NAME" == "main" ]]
        then
          image=$(cat /workspace/imagename)
          docker push $image
        else
          exit 0
        fi

  - id: get-and-save-artifact-registry-image-digest-if-main
    # This is a workaround to help filter only current image vulnerabilities for dashboard - we need the AR image digest for this.
    name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:latest@sha256:3a24ff5f089d9ce3627306873ef1e1061488a63ae12c0bd0b5c24ec5ee594798'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        if [[ $BRANCH_NAME == "main" ]]
        then
          # Get all images from Artifact Registry, sort by create time
          artifact_registry_digests="$(gcloud artifacts files list \
            --project=phx-01j1tbke0ax \
            --repository=phx-01j1tbke0ax-safeinputs \
            --location=northamerica-northeast1 \
            --package=api \
            --sort-by=~CREATE_TIME)"

          # Get most recent image's short sha, use in filename and save to bucket
          most_recent_sha=$(echo "${artifact_registry_digests}" | sed -n 's/.*sha256:\([a-f0-9]\{12\}\).*/\1/p' | head -n 1)
          file_name="gs://safe-inputs-devsecops-outputs-for-dashboard/artifact-registry-image-digests/api__${most_recent_sha}.txt"
          echo "${most_recent_sha}" | gsutil cp - "${file_name}"
        else
          exit 0
        fi
options:
  defaultLogsBucketBehavior: REGIONAL_USER_OWNED_BUCKET
